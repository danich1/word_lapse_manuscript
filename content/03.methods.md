# Methods

## Biomedical Corpora Examined

### Pubtator Central

Pubtator Central is an open-access resource containing annotated PubMed abstracts and full text from PubMed Central [@doi:10.1093/nar/gkz389].
Each annotation used entity recognition systems to label specific words and phrases to represent biomedical concepts.
More specifically, these systems consisted of TaggerOne [@doi:10.1093/bioinformatics/btw343] to tag diseases, chemicals, and cell line entities, GNormPlus [@doi:10.1155/2015/918710] to tag 
genes, SR4GN [@doi:10.1371/journal.pone.0038460] to tag species, and tmVar [@doi:10.1093/bioinformatics/btx541] to tag genetic mutations.
We downloaded this resource on March 09, 2022, and processed over 32 million different documents.
These documents date back to the pre-1800s; however, due to the low sample size, we only used papers published from 2000 to 2021.
We sorted each document based on its publication year and then preprocessed every document using spacy's en_core_web_sm model [@spacy2].
The first step of our preprocessing pipeline replaced each tagged word or phrase with its corresponding entity type and entity id for every sentence.
Next, each sentence is broken down into individual tokens that are normalized into their root form via a process called lemmatization.
After preprocessing, we used all sentences to train multiple models designed to model words based on their context.

### Biomedical Preprints

BioRxiv [@doi:10.1101/833400v1] and MedRxiv [@doi:10.1126/science.aay2933] are repositories that contains preprints for the life science community.
MedRxiv mainly focuses on preprints that mention patient research, while BioRxiv focuses on general biology.
We downloaded a snapshot of both resources on March 4th, 2022, using their respective Amazon S3 bucket [@https://www.biorxiv.org/tdm; @https://www.medrxiv.org/tdm].
This snapshot contained 172,868 BioRxiv preprints and 37,517 MedRxiv preprints.
We sorted each preprint into their respective publication year before processing.
Unlike Pubtator Central, these preprints do not contain any annotations.
Therefore, we used TaggerOne [@doi:10.1093/bioinformatics/btw343] to tag every chemical and disease entity and GNormplus [@doi:10.1155/2015/918710] to tag every gene and species entity for all downloaded preprints.
Once tagged, we used spacy preprocess every preprint.
Akin to Pubtator Central preprocessing, we replaced each tagged word or phrase with its corresponding entity type and entity id.
Then, we used spacy to break down each sentence into lemmatized tokens for downstream analyses.

## Constructing Word Embeddings for Semantic Change Detection

Word2vec [@arxiv:1301.3781] is a natural language processing model designed to model words based on their respective neighbors in the form of dense vectors.
This suite of models comes in two forms, a skipgram model and a continuous bags of words (CBOW) model.
The skipgram model generates these vectors by having a shallow neural network predict a word's neighbors given the word, while the CBOW model predicts the word given its neighbors.
We used the CBOW model to construct these word vectors for this project; however, the skipgram model can also be used.
These word2vec models have stability issues within year and across years [@arxiv:1804.09692; @doi:10.1007/978-3-030-03991-2_73; @doi:10.1162/tacl_a_00008; @pierrejean].
We took an intra-year and inter-year approach to correct for this instability.
For each year, we trained ten different CBOW models using the following parameters: vector size of 300, 10 epochs, minimum frequency cutoff of 5, and a window size of 16 for abstracts.
Every model has its own unique embedding space following training, making it difficult to compare two models without a correction step.
We used an alignment technique termed Orthogonal Procrustes [@doi:10.1007/BF02289451] to account for this issue.
We aligned all trained word2vec models for both datasets to the first model trained in the year 2021.
After alignment, we used each corrected model to detect changes in a word's neighboring context.

## Detecting semantic changes across time

Semantic change is defined as the process where a word's neighborhood undergoes a notable change. 
These events are often detected through various time series analysis techniques [@doi:10.1145/2736277.2741627].
We constructed a time series sequence for every token by calculating its distance within a given year (intra-year) and across each year (inter-year).
Regarding the intra-year distance, we generated the combination of every model pars trained in the same year.
Next, we calculated the cosine distance between each token and its corresponding counterpart. 
Cosine distance is a metric bounded between zero and two, where a score of zero means two vectors are the same, and a score of two means both vectors are different.
For the inter-year distance, we took the cartesian product of every model between two years.
Once paired, we performed the same task of calculating the cosine distance between each token and its corresponding counterpart. 
We calculated the final distance ratio by taking the average inter-year distance over the average intra-year distance.
Under this approach, tokens with high intra-year stability will be penalized and vice-verse for more stable tokens.

Including token frequency with the cosine distance ratio improves results compared to distance alone [@doi:10.1007/s00799-019-00271-6].
We calculated token frequency as the ratio of token frequency in the more recent year over the frequency in the previous year. 
We combined the frequency and distance metrics to make the final sequence following calculations.

Once the time series has been constructed, the final step is to perform change point detection.
This process involves using statistical techniques to detect abnormalities within a given time series.
We used the CUSUM algorithm [@gustafsson] to detect these abnormalities.
This algorithm uses a rolling sum of the differences between two timepoints and checks whether the sum is greater than a threshold.
If the sum is greater than a threshold, this algorithm says the timepoint is considered a changepoint.
We devised this threshold by treating each timepoint as unique and calculated the 99th percentile using every timepoint.
Lastly, we ran the CUSUM algorithm using this calculated threshold,  a drift of 0, and default settings for all other parameters.

