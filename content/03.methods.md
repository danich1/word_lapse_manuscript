# Methods

## Biomedical Corpora Examined

### Pubtator Central

Pubtator Central is an open-access resource containing annotated PubMed abstracts and full text from PubMed Central [@doi:10.1093/nar/gkz389].
Each annotation used entity recognition systems to label specific words and phrases to represent biomedical concepts.
More specifically, this resource used TaggerOne [@doi:10.1093/bioinformatics/btw343] to tag diseases, chemicals, and cell line entities, GNormPlus [@doi:10.1155/2015/918710] to tag 
genes, SR4GN [@doi:10.1371/journal.pone.0038460] to tag species, and tmVar [@doi:10.1093/bioinformatics/btx541] to tag genetic mutations.
We downloaded this resource on March 09, 2022, and processed over 32 million different documents.
These documents date back to the pre-1800s; however, due to the low sample size, we only used papers published from 2000 to 2021.
We sorted each document based on its publication year and then preprocessed every document using spacy's en_core_web_sm model [@spacy2].
We first replaced each tagged phrase with its corresponding entity type and previously identified id for each sentence.
Next, this model breaks each sentence down into individual tokens that are normalized into their root form via a process called lemmatization.
After processing, we used all sentences to train multiple models that are designed to model words based on their context.

### Biomedical Preprints

BioRxiv [@doi:10.1101/833400v1] and MedRxiv [@doi:10.1126/science.aay2933] are repositories that contains preprints for the life science community.
MedRxiv mainly focuses on preprints that mention patient research, while BioRxiv focuses on general biology.
We downloaded a snapshot of both resources on March 4th, 2022, using their respective Amazon S3 bucket [@https://www.biorxiv.org/tdm; @https://www.medrxiv.org/tdm].
This snapshot contained 172,868 BioRxiv preprints and 37,517 MedRxiv preprints.
We sorted each preprint into their respective publication year before processing.
Unlike Pubtator Central, these preprints do not contain any annotations.
Therefore, we used TaggerOne [@doi:10.1093/bioinformatics/btw343] to tag every chemical and disease entity and GNormplus [@doi:10.1155/2015/918710] to tag every gene and species entity for all downloaded preprints.
Once tagged, we used spacy to break down each paragraph into sentences and replaced each tagged phrase with its corresponding entity type and id.
Lastly, we used spacy to break down each sentence into lemmatized tokens for downstream analyses.

## Constructing Word Embeddings for Semantic Change Detection

Word2vec [@arxiv:1301.3781] is a natural language processing model designed to model words based on their respective neighbors in the form of dense vectors.
This suite of models comes in two forms, a skipgram model and a continuous bags of words (CBOW) model.
The skipgram model generates these vectors by having a shallow neural network predict a word's neighbors given the word, while the CBOW model predicts the word given its neighbors.
We used the CBOW model to construct these vectors for this project.
These models have stability issues within year and across years [@arxiv:1804.09692; @doi:10.1007/978-3-030-03991-2_73; @doi:10.1162/tacl_a_00008; @pierrejean], so we took an intra and inter year approach.
For each year, we trained ten different CBOW models using the following parameters: vector size of 300, 10 epochs, minimum frequency cutoff of 5, and a window size of 16 for abstracts.
Following training, each word2vec model has its own unique embedding space.
To allow for direct vector comparison for every trained model, we used an alignment technique termed Orthogonal Procrustes [@doi:10.1007/BF02289451].
We aligned all models to the first trained model in the most recent year using this technique.
After alignment, We used each corrected model to detect changes in a word's neighboring context. 

## UMAP visualization

UMAP is a data visualization tool designed to project high data into a low dimensional space [@sainburg2021parametric].
We used this algorithm to project words from various years into a low dimensional space to confirm alignment.
We trained this model using the following parameters: cosine distance metric, 25 nearest neighbors, random seed of 100, minimum distance of 0.5 and 2 training epochs.

## Detecting semantic changes across time

Semantic change is when a word's neighborhood undergoes a significant change. 
Often, these events are detected through time series analysis [@doi:10.1145/2736277.2741627].
We constructed a time series sequence for every token contained in each trained word2vec model for this project.
We constructed this sequence by taking the cosine distance between every model within a given year (intra-year) and across each year (inter-year).
Cosine distance is a metric bounded between zero and two, where zero means two vectors are the same and two means two vectors are different.
Once calculated, we took the ratio of the average inter-year distance for each datapoint over the average intra-year distance.
Under this approach, tokens with high intra-year stability will be penalized and vice-verse for more stable tokens.
Including token frequency with the cosine distance ratio improves results compared to distance alone [@doi:10.1007/s00799-019-00271-6].
We took the ratio of token frequency between each year and combined it with the cosine distance to make our final metric of change.
We used the CUSUM algorithm [@gustafsson] to perform changepoint detection.
This algorithm uses a rolling sum of the differences between two timepoints and checks whether the sum is greater than a threshold.
If the sum is greater than a threshold, a changepoint is detected.
We devised our threshold by collapsing every change ratio into a single distribution and calculating the 99th  percentile of that distribution.
Once found, we used this threshold, a drift of 0, and default settings for all other parameters to run CUSUM.

