# Methods

## Biomedical Corpora Examined

### Pubtator Central

Pubtator Central is an open-access resource containing annotated abstracts and full-texts with entity recognition systems for biomedical concepts [@doi:10.1093/nar/gkz389].
The systens used were TaggerOne [@doi:10.1093/bioinformatics/btw343] to tag diseases, chemicals, and cell line entities, GNormPlus [@doi:10.1155/2015/918710] to tag genes, SR4GN [@doi:10.1371/journal.pone.0038460] to tag species, and tmVar [@doi:10.1093/bioinformatics/btx541] to tag genetic mutations.
We initially downloaded this resource on December 07th, 2021 and processed over 30 million documents.
This resource contains documents from the pre-1800s to 2021; however, due to the low sample size in the early years, we only used documents published from 2000 to 2021.
The resource was subsequently updated with documents from 2021.
We also downloaded a later version on March 09th, 2022 and merged both versions using each document's doc_id field to produce the corpus used in this analysis.
We divided documents by publication year and then preprocessed each using spacy's en_core_web_sm model [@spacy2].
We replaced each tagged word or phrase with its corresponding entity type and entity ID for every sentence that contained an annotation.
Then, we used spacy to break sentences into individual tokens and normalized each token to its root form via lemmatization.
After preprocessing, we used every sentence to train multiple Natural Language Processing (NLP) models designed to represent words based on their context.

### Biomedical Preprints

We downloaded a snapshot of BioRxiv [@doi:10.1101/833400] and MedRxiv [@doi:10.1126/science.aay2933] on March 4th, 2022, using their respective Amazon S3 buckets [@https://www.biorxiv.org/tdm; @https://www.medrxiv.org/tdm].
This snapshot contained 172,868 BioRxiv and 37,517 MedRxiv preprints.
We filtered each preprint to its most recent version to prevent duplication bias and sorted them into their respective posted year.
Unlike Pubtator Central, these filtered preprints did not contain any annotations.
Therefore, we used TaggerOne [@doi:10.1093/bioinformatics/btw343] to tag chemical and disease entities, and GNormplus [@doi:10.1155/2015/918710] to tag gene and species entities for our preprint set.
We then used spacy to preprocess every preprint as described in the Pubtator Central section.

## Constructing Word Embeddings for Semantic Change Detection

![
A. The first step of our data pipeline is where PMCOA papers and BioRxiv/MedRxiv preprints are binned by their respective posting year.
Following the binning process, we train ten word2vec models for each year's manuscripts.
B. Upon training each individual word2vec model, we align every model onto an anchor model.
C. We capture token differences using an intra-year and inter-year approach.
Each arrow indicates comparing all tokens from one model with their respective selves in a different model.
D. The last step combines the above calculations into a single metric to allow for a time series to be constructed.
Once constructed, we use a statistical technique to autodetect the presence of a changepoint.
](images/methods-pipeline/word-lapse-pipeline.png){#fig:pipeline_overview width="100%}

We used the Word2vec model [@arxiv:1301.3781] to construct word vectors for each year.
This model is a natural language processing model designed to represent words based on their respective neighbors  as dense vectors.
The skipgram model generates these vectors by having a shallow neural network predict a word's neighbors given the word, while the CBOW model predicts the word given its neighbors.
We used the CBOW model to construct word vectors for each year.
Despite the power of these word2vec models, these models are known to differ due to randomization within a year and year-to-year variability across years [@arxiv:1804.09692; @doi:10.1007/978-3-030-03991-2_73; @doi:10.1162/tacl_a_00008; @doi:10.18653/v1/S18-2019].
To control for run-to-run variability, we examined both intra-year and inter-year relationships.
We trained ten different CBOW models for each year using the following parameters: vector size of 300, 10 epochs, minimum frequency cutoff of 5, and a window size of 16 for abstracts (Figure {@fig:pipeline_overview}A).
Every model has its own unique vector space following training, making it difficult to compare two models without a correction step.
We then used orthogonal Procrustes [@doi:10.1007/BF02289451] to align all trained CBOW models for the Pubtator Central dataset to the first model trained in 2021, and all CBOW models for the BioRxiv/MedRxiv dataset to the first model trained in 2021 (Figure {@fig:pipeline_overview}B).
To visualize the aligned models, we used UMAP [@arxiv:1802.03426] with the cosine distance metric, a random_state of 100, 25 for n_neighbors, a minimum distance of 0.99, and 50 n_epochs.


## Detecting semantic changes across time

Once the word2vec models were aligned, the next step was to detect semantic change.
Semantic change events were detected through time series analysis [@doi:10.1145/2736277.2741627].
We constructed a time series sequence for each token by calculating its distance within a given year (intra-year) and across each year (inter-year) (Figure {@fig:pipeline_overview}C).
We used the model pairs constructed from the same year to calculate an intra-year distance, which was the cosine distance between each token and its corresponding counterpart.
The cosine distance is a metric bounded between 0 and 2, where a score of 0 indicates that two vectors are the same, and a score of 2 indicates that the two vectors are different.
For the inter-year distance, we used the Cartesian product of every model between two years and calculated the distance between tokens in the same way as the the intra-year distance.
We then combined both metrics by taking the ratio of the average inter-year distance over the average intra-year distance.
This approach penalizes tokens with high intra-year instability and rewards more stable tokens.
Additionally, it has been shown that including token frequency improves results compared to using distance alone [@doi:10.1007/s00799-019-00271-6].
We calculated token frequency as the ratio of token frequency in the more recent year over the frequency of the previous year.
Finally, we combined the frequency with the distance ratios to make the final metric (Figure {@fig:pipeline_overview}D).

Following time series construction, we performed change point detection, which uses statistical techniques to detect abnormalities within a given time series (Figure {@fig:pipeline_overview}D).
We used the CUSUM algorithm [@gustafsson], which uses a rolling sum of the differences between two timepoints and checks whether the sum is greater than a threshold.
A change point is considered to have occurred if the sum exceeds a threshold.
We used the 99th percentile on every generated timepoint as the threshold, and ran the CUSUM algorithm with a drift of 0 and default settings for all other parameters.
