# Methods

## Biomedical Corpora Examined

### Pubtator Central

Pubtator Central is an open access resource that contains annotated PubMed abstracts and full text from PubMed Central [@doi:10.1093/nar/gkz389].
Each annotation used entity recognistion systems to label specific words and phrases to represent biomedical concepts.
More speficially, this resource used TaggerOne [@doi:10.1093/bioinformatics/btw343] to tag diseases, chemicals and cell line entities, GNormPlus [@doi:10.1155/2015/918710] to tag 
genes, SR4GN [@doi:10.1371/journal.pone.0038460] to tag species, and tmVar [@doi:10.1093/bioinformatics/btx541] to tag genetic mutations.
We downloaded this resource on March 09, 2022 and processed over 32 million different documents.
These documents date all the back to the pre-1800s; however, due to the low sample size, we only used documents published in the year 2000 to 2021.
We sorted each document into its respective publication year and then preocessed every document using spacy's en_core_web_sm model [@spacy2].
This model breaks down every paragraph into individual sentences.
For each sentnece we first replaced each tagged phrase with it's corresponding entity type and id that was previously identified.
Next, this model breaks each sentence down into individual tokens that are normlaized into their root from using a process called lemmatization.
After processing, we used all sentences to train multiple models that are designed to model words based on their context.

### Biomedical Preprints

BioRxiv [@doi:10.1101/833400v1] and MedRxiv [@doi:10.1126/science.aay2933] are repositories that contains preprints for the life science community.
MedRxiv is mainly focused on preprints that mention patient research, while BioRxiv is focused on general biology.
We downloaded a snapshot of both resources on March 4th, 2022, using their respective Amazon S3 bucket [@https://www.biorxiv.org/tdm; @https://www.medrxiv.org/tdm].
This snapshot contained 172,868 BioRxiv preprints and 37,517 MedRxiv preprints.
We sorted each preprint into their respective publication year  before processing.
Unlike Pubtator Central, these preprints do not contain any annotations.
Therefore, we used TaggerOne [@doi:10.1093/bioinformatics/btw343] to tag every chemical and disease entity and GNormplus [@doi:10.1155/2015/918710] to tag every gene and species entity for all downloaded preprints.
Once tagged, we used spacy to break down each paragraph into sentences and replaced each tagged phrased with their corresponding entity type and id.
Lastly, we used spacy to break down each sentence into lemmatized tokens to be used for downstream analyses.

## Constructing Word Embeddings for Semantic Change Detection

Word2vec [@arxiv:1301.3781] is a natural language processing model designed to model words based on their reespective neighbors in the form of dense vectors.
This suite of models comes in two forms, a skipgram model and a continuous bags of words (CBOW) model.
The skipgram model generates these vectors by having a shallow neural network predict a word's neighbors given the word, while the CBOW model predicts the word given its neighbors.
We used the CBOW model to construct these vectors for this project.
These models have an issue with stability within year and across years [@arxiv:1804.09692; @doi:10.1007/978-3-030-03991-2_73; @doi:10.1162/tacl_a_00008; @pierrejean], so we took an intra and inter year approach.
For each year, we trained ten different CBOW modles using the following parameters: vector size of 300, 10 epochs, minimum frequency cutoff of 5 and a window size of 16 for abstracts.
Following training, each word2vec model has their own unique embedding space.
To allow for direct vector comparison for every trained model we used an alignment technique, termed orthogonal procrustes [@doi:10.1007/BF02289451].
Using this technique, we aligned all models to the first trained model in the most recent year.
Following aligment, we used each corrected model to detect changes in a word's neighboring context. 

## Detecting semantic changes across time

Semantic change is defined as a timepoint where a word's neighborhood underwent a significant change. 
These timepoints are often detected through time series analysis [@doi:10.1145/2736277.2741627].
For this project, we constructed a time series sequence for every token contained in each trained word2vec model.
We constructed this sequence by taking the cosine distance between every model within a given year (intra) and across each year (inter).
Cosine distance is a metric thats bounded between 0 and 2, where 0 means two vectors are the same and 2 means two vectors are different.
Once calculated, for each datapoint we took the ratio of the average inter year distance over  the average intra year distance.
Under this approach, tokens that have higher intra-year stability will be penalized and vice-verse for more stable tokens.
Along with the cosine distance ratio, it has been shown that including token frequency can improve results [@doi:10.1007/s00799-019-00271-6].
In conjunction with distance, we took the ratio of token frequency between each years and combined it with the cosine distance to make the final metric of change.
We used the CUSUM algorithm [@gustafsson] to perform changepoiint detection.
This algorithm uses rolling sum of the differences between two timepoints and checks to see if the sum is greater than a threshold.
If the sum is greater than a threshold then a changepoint is detected.
We ran this algorithm using a threshold of <insert threshold here when finished>, and a drift of 0.


## UMAP visualization

mention projection of words onto a 2D space

5. Umap visualization
   1. Explain why aligned umap - preserves local and global structure
   2. mention parameters for aligned umap
