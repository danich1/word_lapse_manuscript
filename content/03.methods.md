# Methods

## Biomedical language examined

### Pubtator Central

Pubtator Central is an open access resource that contains preannotated biomedical abstracts and full text [@doi:10.1093/nar/gkz389].
Each annotation consists of specific words and phrases tagged to represent biomedical concepts such as genes, diseases, chemicals, species and cell lines.

<div style="color:red">
mention the models used to tag concepts here
</div>

We downloaded this resource on June 19, 2021 and processing over 32 million different documents.
We preprocessed every sentence in Pubtator Central using spacy's en_core_web_sm model [].
This model breaks every english word down in their root form through a process called lemmatization.
Following this process, we replaced each tagged phrase with it's corresponding entity type that was previously identified.


### BioRxiv

This may need to be described if the preliminary data is promising.

## Word Embeddings to capture word context

Word2vec [@arxiv:1301.3781] is an natural language processing model designed to model word context in the form of dense vectors.
This suite of models comes in two forms, a skipgram model and a continuous bags of words (CBOW) model.
The skipgram model models word context by having a neural network predicted the context given the word, while the CBOW model predicts the word given the context.
We used the CBOW model to capture this context for this project.
We split Pubmed Central dataset into abstracts and full text.
We trained ten different CBOW for vector size of 300, 10 epochs, minimum frequency cutoff of 10 and a window size of 16 for abstracts.
Full text provides a lot more sentences and we trained ten different word2vec models using the following parameters, vector size of 300, 10 epochs, minimum frequency cutoff of 10 and a window size of 16.

Despite the power of word2vec, these models generate their low dimensional constructs arbitrarily.
Based on this these models cannot be directly compared without an alignment step.
We used orthogonal procrustes [@doi:10.1007/BF02289451] to align every model to the first index of the most recent year.

## Detecting Semantic Change

Mention cosine similarity and how that metric works
Mention how you use cosine similarity to calculate intra year variation (within year) and inter year variation (across year)
Mention how you turn these values into a ratio metric and that you are adding the ratio metric with frequency
Mention how you use the CUSUM algorithm to detect the abnormal shifts.

## UMAP visualization

mention projection of words onto a 2D space

1. Pubtator Central
   1. breif description about the dataset
   2. talk about how it contains entities tagged
2. Word2vec Model
   1. training parameters
   2. Use 10 models for each year
   3. min cutoff is 10
3. Orthogonal Procrustes - to align models onto year 2021
   1. Allows for the models to be directly compared
4. Determining semantic change
	1. Cosine metric to determine difference between words
	2. Scaf ratio method to model temporal changes
	3. CUSUM to actually detect change throughout the years
5. Umap visualization
   1. Explain why aligned umap - preserves local and global structure
   2. mention parameters for aligned umap
